{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, csv,time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import statistics\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('data/preprocessed_reviewinfo.csv')\n",
    "main_df['processed_content'] = ''\n",
    "main_df.drop(['Title', 'Author', 'ReviewID', 'Overall', 'Date', 'polarity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing and building classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT RUN THIS CODE IF YOU HAVE NO TIME ### REFER TO THE CODE BELOW TO GET THE PROCESSED REVIEWS ###\n",
    "# Processsing for stopwords, alphabetic words, Stemming \n",
    "start = time.time()\n",
    "\n",
    "# lower case all the words in the doc\n",
    "main_df['Content'] = [doc.lower() for doc in main_df['Content']]\n",
    "\n",
    "#tokenize the words\n",
    "main_df['Content']= [word_tokenize(doc) for doc in main_df['Content']]\n",
    "\n",
    "# Remove all non-words tokens\n",
    "main_df['Content'] = [[w for w in doc if re.search('^[a-z]+$',w)] for doc in main_df['Content']]\n",
    "\n",
    "# Removing stop words\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "main_df['Content'] = [[w for w in doc if w not in stop_list] for doc in main_df['Content']]\n",
    "\n",
    "# Stemming on words\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_doc = [[stemmer.stem(w) for w in doc] for doc in main_df['Content']]\n",
    "\n",
    "#store the processed doc\n",
    "index = 0\n",
    "for doc in stemmed_doc:\n",
    "    main_df.loc[index,'processed_content'] = str(doc)\n",
    "    index = index + 1\n",
    "\n",
    "end = time.time()\n",
    "print(\"time taken: \" + str((end - start)) + \" secs\")\n",
    "\n",
    "# Export out the processed reviews to avoid processing again\n",
    "main_df.to_csv(r'data/processed_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and validate (via reload of data from processed reviews)\n",
    "main_df = pd.read_csv('data/processed_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['category', 'Content', 'processed_content'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(main_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into train and validate\n",
    "df, validate_set = train_test_split(main_df, test_size=0.20, random_state=0)\n",
    "\n",
    "# Conduct oversampling of data to counter imbalance dataset. Only oversample trainingset\n",
    "df.sort_values(by=['category'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "counts = df.groupby('category').size()\n",
    "\n",
    "Oversample cameras\n",
    "num_to_add_cameras = counts[2] - counts[0]\n",
    "df_train_cameras = df[df['category']=='cameras']\n",
    "df_train_cameras = df_train_cameras.append(df_train_cameras.loc[0:num_to_add_cameras-1])\n",
    "\n",
    "# Oversample laptops\n",
    "df_train_laptops = df[df['category']=='laptops']\n",
    "df_train_laptops = df_train_laptops.append(df_train_laptops)\n",
    "num_to_add_laptops = counts[2] - len(df_train_laptops)\n",
    "df_train_laptops.reset_index(drop=True, inplace=True)\n",
    "df_train_laptops = df_train_laptops.append(df_train_laptops.loc[0:(num_to_add_laptops-1)])\n",
    "\n",
    "# There is no need to oversample mobile phone since it has the largest amount\n",
    "df_train_mobile_phone = df[df['category']=='mobile phone']\n",
    "\n",
    "# Append them to make a new training dataset\n",
    "df = df_train_cameras.append(df_train_laptops.append(df_train_mobile_phone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate encoder\n",
    "Encoder = LabelEncoder()\n",
    "\n",
    "# Create the dictionary in TFIDF\n",
    "# There are too many unique words. Set max features to 5000\n",
    "TFIDF_vect = TfidfVectorizer(max_features=5000)\n",
    "TFIDF_vect.fit(main_df['processed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the feature extration to avoid training again\n",
    "### ONLY RUN THIS TO SAVE feature extration ###\n",
    "\n",
    "save_FE = open(\"model_classification/TFIDF_Reviews_Category.pickle\",\"wb\")\n",
    "pickle.dump(TFIDF_vect, save_FE)\n",
    "save_FE.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the saved feature extration\n",
    "FE_saved = open(\"model_classification/TFIDF_Reviews_Category.pickle\", \"rb\")\n",
    "TFIDF_vect = pickle.load(FE_saved)\n",
    "FE_saved.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 (NB Bernoulli): 0.8550087873462214\n",
      "Iteration 1 (NB Multinomial): 0.9404217926186291\n",
      "Iteration 1 (SVM): 0.9674868189806678\n",
      "Iteration 1 (LogR): 0.9689806678383128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 (NB Bernoulli): 0.8566783831282953\n",
      "Iteration 2 (NB Multinomial): 0.9366432337434095\n",
      "Iteration 2 (SVM): 0.9695957820738137\n",
      "Iteration 2 (LogR): 0.969859402460457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 (NB Bernoulli): 0.8616871704745167\n",
      "Iteration 3 (NB Multinomial): 0.9408611599297012\n",
      "Iteration 3 (SVM): 0.9672231985940246\n",
      "Iteration 3 (LogR): 0.968804920913884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4 (NB Bernoulli): 0.8565026362038665\n",
      "Iteration 4 (NB Multinomial): 0.9375219683655536\n",
      "Iteration 4 (SVM): 0.9666080843585237\n",
      "Iteration 4 (LogR): 0.9669595782073813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 (NB Bernoulli): 0.8560632688927944\n",
      "Iteration 5 (NB Multinomial): 0.9369068541300527\n",
      "Iteration 5 (SVM): 0.9661687170474517\n",
      "Iteration 5 (LogR): 0.9676625659050967\n"
     ]
    }
   ],
   "source": [
    "#Conduct Cross Validation kfold=5 and find the average score of each model\n",
    "\n",
    "# Store the scores \n",
    "NB_Bernoulli_scores = []\n",
    "NB_Multinomial_scores = []\n",
    "LogR_scores = []\n",
    "SVM_scores = []\n",
    "count = 1\n",
    "\n",
    "# Store the time taken to build and predict\n",
    "NB_Bernoulli_build = []\n",
    "NB_Multinomial_build = []\n",
    "LogR_build = []\n",
    "SVM_build = []\n",
    "NB_Bernoulli_predict = []\n",
    "NB_Multinomial_predict = []\n",
    "LogR_predict = []\n",
    "SVM_predict = []\n",
    "\n",
    "# Instantiate cross validation folds\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.20, random_state=0)\n",
    "\n",
    "for train_index, test_index in ss.split(df):\n",
    "    x_train = df.iloc[train_index, 2] #the 4 partitions, processed_content column\n",
    "    y_train = df.iloc[train_index, 0] #the 4 partitions, category column\n",
    "    \n",
    "    x_test = df.iloc[test_index, 2] #the 1 partitions to test, processed_content column\n",
    "    y_test = df.iloc[test_index, 0] #the 1 partition to test, category column\n",
    "    \n",
    "    # Convert categories to cameras:0, laptops:1, mobile phone:2\n",
    "    y_train = Encoder.fit_transform(y_train)\n",
    "    y_test = Encoder.fit_transform(y_test)\n",
    "    \n",
    "    # Transform reviews into TDIFD\n",
    "    x_train_TFIDF = TFIDF_vect.transform(x_train)\n",
    "    x_test_TFIDF = TFIDF_vect.transform(x_test)\n",
    "    \n",
    "    # Build the classifiers\n",
    "    \n",
    "    start = time.time()\n",
    "    NB_bernoulli_clf = naive_bayes.BernoulliNB()\n",
    "    NB_bernoulli_clf.fit(x_train_TFIDF, y_train)\n",
    "    NB_Bernoulli_build.append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    NB_Multinomial_clf = naive_bayes.MultinomialNB()\n",
    "    NB_Multinomial_clf.fit(x_train_TFIDF, y_train)\n",
    "    NB_Multinomial_build.append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    SVM_clf = svm.SVC(kernel='linear')\n",
    "    SVM_clf.fit(x_train_TFIDF, y_train)\n",
    "    SVM_build.append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    LogR_clf = LogisticRegression()\n",
    "    LogR_clf.fit(x_train_TFIDF, y_train)\n",
    "    LogR_build.append(time.time() - start)\n",
    "    \n",
    "    # Predict and generate score\n",
    "    start = time.time()\n",
    "    y_pred_1 = NB_bernoulli_clf.predict(x_test_TFIDF)\n",
    "    NB_bernoulli_score = accuracy_score(y_pred_1, y_test)\n",
    "    NB_Bernoulli_predict.append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    y_pred_2 = NB_Multinomial_clf.predict(x_test_TFIDF)\n",
    "    NB_Multinomial_score = accuracy_score(y_pred_2, y_test)\n",
    "    NB_Multinomial_predict.append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    y_pred_3 = SVM_clf.predict(x_test_TFIDF)\n",
    "    SVM_score = accuracy_score(y_pred_3, y_test)\n",
    "    SVM_predict.append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    y_pred_4 = LogR_clf.predict(x_test_TFIDF)\n",
    "    LogR_score = accuracy_score(y_pred_4, y_test)\n",
    "    LogR_predict.append(time.time() - start)\n",
    "    \n",
    "    # Store the score\n",
    "    NB_Bernoulli_scores.append(NB_bernoulli_score)\n",
    "    NB_Multinomial_scores.append(NB_Multinomial_score)\n",
    "    SVM_scores.append(SVM_score)\n",
    "    LogR_scores.append(LogR_score)\n",
    "    \n",
    "    print(\"Iteration \" + str(count) + \" (NB Bernoulli): \" + str(NB_bernoulli_score))\n",
    "    print(\"Iteration \" + str(count) + \" (NB Multinomial): \" + str(NB_Multinomial_score))\n",
    "    print(\"Iteration \" + str(count) + \" (SVM): \" + str(SVM_score))\n",
    "    print(\"Iteration \" + str(count) + \" (LogR): \" + str(LogR_score))\n",
    "    \n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Bernoulli Accuracy from Cross Validation: 0.8571880492091388\n",
      "NB Multinomial Accuracy from Cross Validation: 0.9384710017574692\n",
      "LogR Accuracy from Cross Validation: 0.9684534270650264\n",
      "SVM Accuracy from Cross Validation: 0.9674165202108963\n",
      "\n",
      "NB Bernoulli time taken to build: 0.2270498275756836 secs\n",
      "NB Multinomial time taken to build: 0.1380290985107422 secs\n",
      "LogR Accuracy time taken to build: 9.730051517486572 secs\n",
      "SVM Accuracy time taken to build: 446.7202858924866 secs\n",
      "\n",
      "NB Bernoulli time taken to predict: 0.042009830474853516 secs\n",
      "NB Multinomial time taken to predict: 0.017003297805786133 secs\n",
      "LogR Accuracy time taken to predict: 0.015002727508544922 secs\n",
      "SVM Accuracy time taken to predict: 66.76188111305237 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"NB Bernoulli Accuracy from Cross Validation: \" + str(statistics.mean(NB_Bernoulli_scores)))\n",
    "print(\"NB Multinomial Accuracy from Cross Validation: \" + str(statistics.mean(NB_Multinomial_scores)))\n",
    "print(\"LogR Accuracy from Cross Validation: \" + str(statistics.mean(LogR_scores)))\n",
    "print(\"SVM Accuracy from Cross Validation: \" + str(statistics.mean(SVM_scores)) + \"\\n\")\n",
    "\n",
    "print(\"NB Bernoulli time taken to build: \" + str(sum(NB_Bernoulli_build)) + \" secs\")\n",
    "print(\"NB Multinomial time taken to build: \" + str(sum(NB_Multinomial_build)) + \" secs\")\n",
    "print(\"LogR Accuracy time taken to build: \" + str(sum(LogR_build)) + \" secs\")\n",
    "print(\"SVM Accuracy time taken to build: \" + str(sum(SVM_build)) + \" secs\\n\")\n",
    "\n",
    "print(\"NB Bernoulli time taken to predict: \" + str(sum(NB_Bernoulli_predict)) + \" secs\")\n",
    "print(\"NB Multinomial time taken to predict: \" + str(sum(NB_Multinomial_predict)) + \" secs\")\n",
    "print(\"LogR Accuracy time taken to predict: \" + str(sum(LogR_predict)) + \" secs\")\n",
    "print(\"SVM Accuracy time taken to predict: \" + str(sum(SVM_predict)) + \" secs\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and validate (via reload of data from processed reviews)\n",
    "main_df = pd.read_csv('data/processed_reviews.csv')\n",
    "df, validate_set = train_test_split(main_df, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.9701933216168717\n"
     ]
    }
   ],
   "source": [
    "#BASED ON CROSS VALIDATION, LogisticRegression HAS THE BEST EFFICIENCY FOR RESULT\n",
    "\n",
    "# Use the model with the best score from cross validation of models\n",
    "# Then, test on validate_set and find the final score\n",
    "\n",
    "# Transform reviews into TDIFD\n",
    "x_train = df['processed_content']\n",
    "x_train_TFIDF = TFIDF_vect.transform(x_train)\n",
    "x_validate = validate_set['processed_content']\n",
    "x_validate_TFIDF = TFIDF_vect.transform(x_validate)\n",
    "\n",
    "# Convert categories to cameras:0, laptops:1, mobile phone:2\n",
    "y_train = df['category']\n",
    "y_train = Encoder.fit_transform(y_train)\n",
    "y_validate = validate_set['category']\n",
    "y_validate = Encoder.fit_transform(y_validate)\n",
    "\n",
    "# Build the classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train_TFIDF, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = classifier.predict(x_validate_TFIDF)\n",
    "NB_Multinomial_score = accuracy_score(y_pred, y_validate)\n",
    "print(\"Final Accuracy: \" + str(NB_Multinomial_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how well the classifier performs in predicting each of the category\n",
    "matrix = confusion_matrix(y_pred, y_validate)\n",
    "\n",
    "cameras_true = matrix[0][0]\n",
    "cameras_wrong_as_laptops = matrix[1][0]\n",
    "cameras_wrong_as_mobilephone = matrix[2][0]\n",
    "accuracy_for_cameras = cameras_true / (cameras_true + cameras_wrong_as_laptops + cameras_wrong_as_mobilephone)\n",
    "\n",
    "laptops_true = matrix[1][1]\n",
    "laptops_wrong_as_cameras = matrix[0][1]\n",
    "laptops_wrong_as_mobilephone = matrix[2][1]\n",
    "accuracy_for_laptops = laptops_true / (laptops_true + laptops_wrong_as_cameras + laptops_wrong_as_mobilephone)\n",
    "\n",
    "mobilephone_true = matrix[2][2]\n",
    "mobilephone_wrong_as_laptops = matrix[1][2]\n",
    "mobilephone_wrong_as_cameras = matrix[0][2]\n",
    "accuracy_for_mobilephone = mobilephone_true / (mobilephone_true + mobilephone_wrong_as_laptops + mobilephone_wrong_as_cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for predicting camera: 0.9693500738552437\n",
      "Accuracy for predicting laptop: 0.9299332697807435\n",
      "Accuracy for predicting mobile phone: 0.9834599910594546\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for predicting camera: \" + str(accuracy_for_cameras))\n",
    "print(\"Accuracy for predicting laptop: \" + str(accuracy_for_laptops))\n",
    "print(\"Accuracy for predicting mobile phone: \" + str(accuracy_for_mobilephone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following steps are to generate the predicted labels for each reviews\n",
    "\n",
    "new_df = pd.read_csv('data/preprocessed_reviewinfo.csv')\n",
    "not_used_set, validate_set_1 = train_test_split(new_df, test_size=0.20, random_state=0)\n",
    "validate_set_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# convert predicted reviews into dataframe\n",
    "df_predicted = pd.DataFrame(y_pred, columns=[\"predicted_category\"])\n",
    "\n",
    "# map the predicted values back to original categories\n",
    "category_mapper = {\n",
    "    0: 'cameras',\n",
    "    1: 'laptops',\n",
    "    2: 'mobile phone'\n",
    "}\n",
    "df_predicted['predicted_category'] = df_predicted['predicted_category'].map(category_mapper)\n",
    "\n",
    "# dataframe that contains the predicted and true category for each review\n",
    "df_results = validate_set_1.join(df_predicted)\n",
    "\n",
    "# Export out the reviews with predicted categories to avoid processing again\n",
    "df_results.to_csv(r'data/reviews_with_predicted_categories.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load (Recommend to load instead of running code to get classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the naive bayes classifier to avoid training again\n",
    "### ONLY RUN THIS TO SAVE CLASSIFIER ###\n",
    "\n",
    "save_classifier = open(\"model_classification/CategoryClassifier.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the saved classifier\n",
    "classifier_saved = open(\"model_classification/CategoryClassifier.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_saved)\n",
    "classifier_saved.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction via Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-read the file\n",
    "main_df = pd.read_csv('data/preprocessed_reviewinfo.csv')\n",
    "\n",
    "# Splitting the dataset into train and validate\n",
    "df, validate_set = train_test_split(main_df, test_size=0.20, random_state=0)\n",
    "stop_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Label each reviews with respective category for train set\n",
    "\n",
    "cameras_docs = []\n",
    "laptops_docs = []\n",
    "mobile_phone_docs = []\n",
    "\n",
    "df.sort_values(by=['category'], inplace=True)\n",
    "for index, row in df.iterrows():\n",
    "    if row['category'] == 'cameras':\n",
    "        holder = [word_tokenize(row['Content']), 'cameras']        \n",
    "        cameras_docs.append(holder)\n",
    "        \n",
    "    elif row['category'] == 'laptops':\n",
    "        holder = [word_tokenize(row['Content']), 'laptops'] \n",
    "        laptops_docs.append(holder)\n",
    "        \n",
    "    elif row['category'] == 'mobile phone':\n",
    "        holder = [word_tokenize(row['Content']), 'mobile phone'] \n",
    "        mobile_phone_docs.append(holder)\n",
    "\n",
    "# Combine the documents of each categories into a list\n",
    "all_docs = cameras_docs + laptops_docs + mobile_phone_docs\n",
    "random.shuffle(all_docs)\n",
    "\n",
    "# Take 2000 random reviews and use their words for feature extraction\n",
    "all_words = []\n",
    "for i in range(0, 2000):\n",
    "    all_words = all_words + all_docs[i][0]\n",
    "\n",
    "# Processsing for stopwords, alphabetic words.\n",
    "# We do not stem the word because we want readibility in feature extraction\n",
    "all_words_1 = [w.lower() for w in all_words]\n",
    "all_words_2 = [w for w in all_words_1 if re.search('^[a-z]+$',w)]\n",
    "all_words_3 = [w for w in all_words_2 if w not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 83.79626893997192 secs\n"
     ]
    }
   ],
   "source": [
    "# Build the naive bayes feature extraction model\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "all_words_freq = nltk.FreqDist(w.lower() for w in all_words_3)\n",
    "word_features = list(all_words_freq)[:1000]\n",
    "\n",
    "def document_features(document): \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets_train = [(document_features(d), c) for (d,c) in all_docs]\n",
    "classifier = nltk.NaiveBayesClassifier.train(featuresets_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"time taken: \" + str((end - start)) + \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          contains(lens) = True           camera : laptop =   1115.1 : 1.0\n",
      "      contains(keyboard) = True           laptop : camera =    670.4 : 1.0\n",
      "    contains(chromebook) = True           laptop : mobile =    341.9 : 1.0\n",
      "         contains(canon) = True           camera : mobile =    280.4 : 1.0\n",
      "     contains(telephoto) = True           camera : mobile =    251.7 : 1.0\n",
      "           contains(sim) = True           mobile : camera =    245.3 : 1.0\n",
      "         contains(mouse) = True           laptop : camera =    224.3 : 1.0\n",
      "      contains(contract) = True           mobile : camera =    198.5 : 1.0\n",
      "         contains(nikon) = True           camera : mobile =    180.2 : 1.0\n",
      "       contains(prepaid) = True           mobile : camera =    170.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the key features and put into dataframe\n",
    "\n",
    "list_1 = classifier.most_informative_features(20)\n",
    "df_important_features = pd.DataFrame(columns=['Feature','Category_1',\n",
    "                                              'Category_0','Cat1_Cat0','Ratio','Ratio_1'])\n",
    "\n",
    "for (fname, fval) in list_1:\n",
    "    cpdist = classifier._feature_probdist\n",
    "    \n",
    "    def labelprob(l):\n",
    "        return cpdist[l, fname].prob(fval)\n",
    "\n",
    "    labels = sorted(\n",
    "        [l for l in classifier._labels if fval in cpdist[l, fname].samples()],\n",
    "        key=labelprob\n",
    "    )\n",
    "    \n",
    "    if len(labels) == 1:\n",
    "        continue\n",
    "    l0 = labels[0]\n",
    "    l1 = labels[-1]\n",
    "    if cpdist[l0, fname].prob(fval) == 0:\n",
    "        ratio = 'INF'\n",
    "    else:\n",
    "        ratio = round(cpdist[l1, fname].prob(fval) / cpdist[l0, fname].prob(fval), 1)\n",
    "        fname = fname.replace('contains(','')\n",
    "        fname = fname.replace(')','')        \n",
    "        df_important_features.loc[len(df_important_features)] = [fname, l1, l0, l1+\" : \"+l0, \n",
    "                                                ratio, str(ratio)+\" : 1.0\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export into csv if needed\n",
    "df_important_features.to_csv(r'data/important_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load (Recommend to load instead of running code to get feature extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the feature extractor to avoid training again\n",
    "### ONLY RUN THIS TO SAVE FEATURE EXTRACTOR ###\n",
    "\n",
    "save_classifier = open(\"model_classification/FeatureExtraction.pickle\",\"wb\") #binary write\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the saved classifier \n",
    "classifier_saved = open(\"model_classification/FeatureExtraction.pickle\", \"rb\") #binary read\n",
    "classifier = pickle.load(classifier_saved)\n",
    "classifier_saved.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
